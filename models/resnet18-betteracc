!pip -q install torch torchvision tqdm  

import torch, torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from tqdm import tqdm
import random, time, os

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using", device)

seed = 42
random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

mean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262)

train_tfms = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])
test_tfms  = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

train_ds = datasets.CIFAR10(root="~/data", train=True,  download=True, transform=train_tfms)
test_ds  = datasets.CIFAR10(root="~/data", train=False, download=True, transform=test_tfms)

train_dl = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)
test_dl  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True)

def resnet18_cifar10():
    m = models.resnet18(weights=None)
    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    m.maxpool = nn.Identity()
    m.fc = nn.Linear(512, 10)
    return m

model = resnet18_cifar10().to(device)

optimiser = optim.SGD(model.parameters(), lr=0.1, momentum=0.9,
                      weight_decay=5e-4, nesterov=True)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=50)
scaler = torch.cuda.amp.GradScaler()

def run(model, loader, train=False):
    model.train(mode=train)
    total, correct, running_loss = 0, 0, 0
    pbar = tqdm(loader, leave=False)
    for x, y in pbar:
        x, y = x.to(device), y.to(device)
        with torch.cuda.amp.autocast(), torch.set_grad_enabled(train):
            logits = model(x)
            loss = nn.CrossEntropyLoss()(logits, y)
        if train:
            scaler.scale(loss).backward()
            scaler.step(optimiser); scaler.update(); optimiser.zero_grad()
        running_loss += loss.item() * y.size(0)
        _, pred = logits.max(1)
        correct += pred.eq(y).sum().item()
        total   += y.size(0)
        pbar.set_description(f'{"Train" if train else "Test "} '
                             f'loss {running_loss/total:.3f} '
                             f'acc {100*correct/total:.2f}%')
    return running_loss / total, 100 * correct / total


best_acc = 0
for epoch in range(1, 51):
    t0 = time.time()
    train_loss, train_acc = run(model, train_dl, train=True)
    test_loss,  test_acc  = run(model, test_dl,  train=False)
    scheduler.step()
    dt = time.strftime('%H:%M:%S', time.gmtime(time.time() - t0))

    print(f'Epoch {epoch:02d}/50 | '
          f'train_acc {train_acc:.2f}% | test_acc {test_acc:.2f}% | {dt}')

    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), "resnet18_cifar10_best.pth")

print(f"Best test accuracy: {best_acc:.2f}%")
